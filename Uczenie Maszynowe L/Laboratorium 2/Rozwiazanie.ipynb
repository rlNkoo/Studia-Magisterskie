{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b7ebc3",
   "metadata": {},
   "source": [
    "\n",
    "# Myocardial Infarction Complications — Klasyfikacja\n",
    "**Zbiór danych:** Kaggle — *Myocardial Infarction Complications* (CSV)\n",
    "\n",
    "W tym notatniku zrobisz krok po kroku:\n",
    "1. Wczytanie i szybki przegląd danych  \n",
    "2. Automatyczna detekcja kolumny celu (z możliwością ręcznej zmiany)  \n",
    "3. Podział train/test (stratyfikacja) i przygotowanie danych  \n",
    "4. Modele: **Drzewo decyzyjne**, **Bagging**, **Random Forest**, **XGBoost**  \n",
    "5. Ewaluacja: **accuracy, precision, recall, F1, ROC-AUC**, **macierze pomyłek**, **krzywe ROC/PR**  \n",
    "6. **Feature importance** dla RF/XGB oraz krótkie wnioski\n",
    "\n",
    "> Jeśli `xgboost` nie jest zainstalowany, pokażemy komunikat i pominiesz ten model (reszta działa).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a359f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (opcjonalnie) Zainstaluj brakujące biblioteki lokalnie\n",
    "# !pip install -q pandas numpy scikit-learn matplotlib xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, RocCurveDisplay, PrecisionRecallDisplay,\n",
    "    ConfusionMatrixDisplay, classification_report\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "# XGBoost (opcjonalnie)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe7b3c",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Wczytanie danych\n",
    "Umieść pobrany z Kaggle plik CSV (np. `myocardial_infarction_complications.csv`) **w tym samym folderze co notatnik**.\n",
    "Poniższy kod spróbuje automatycznie znaleźć plik i kolumnę celu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Kandydaci ścieżek do pliku — dodaj tu własną nazwę, jeśli inna\n",
    "DATA_CANDIDATES = [\n",
    "    'myocardial_infarction_complications.csv',\n",
    "    'myocardial-infarction-complications.csv',\n",
    "    'mi_complications.csv',\n",
    "    'data.csv',\n",
    "    './data/myocardial_infarction_complications.csv',\n",
    "    '/mnt/data/myocardial_infarction_complications.csv'\n",
    "]\n",
    "\n",
    "csv_path = None\n",
    "for p in DATA_CANDIDATES:\n",
    "    if os.path.exists(p):\n",
    "        csv_path = p\n",
    "        break\n",
    "\n",
    "if csv_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Nie znaleziono pliku CSV. Zmień listę DATA_CANDIDATES powyżej albo dodaj plik obok notatnika.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Wczytano dane z: {csv_path} — shape={df.shape}\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7aaa9",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Automatyczna detekcja kolumny celu\n",
    "Notatnik spróbuje znaleźć kolumnę celu (np. *complication*, *outcome*, *death*, *mortality*, *readmission* itp.).\n",
    "Jeśli wykrycie się nie powiedzie — **ustaw ręcznie nazwę w zmiennej `TARGET`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994524ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spróbuj znaleźć target po nazwie\n",
    "LOWER_COLS = {c.lower(): c for c in df.columns}\n",
    "PATTERNS = [\n",
    "    r'complic', r'outcome', r'target', r'class', r'label', r'death', r'mortal', r'event', r'adverse'\n",
    "]\n",
    "\n",
    "found = None\n",
    "for pat in PATTERNS:\n",
    "    for lc, orig in LOWER_COLS.items():\n",
    "        if re.search(pat, lc):\n",
    "            found = orig\n",
    "            break\n",
    "    if found:\n",
    "        break\n",
    "\n",
    "# Jeśli nie znalazło, użyj ostatniej kolumny jako domysł (częsta konwencja w niektórych CSV)\n",
    "TARGET = found if found is not None else df.columns[-1]\n",
    "print(\"Wykryta/założona kolumna celu:\", TARGET)\n",
    "print(\"Unikalne wartości celu:\", pd.Series(df[TARGET]).value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Jeśli chcesz ręcznie: odkomentuj i podaj nazwę\n",
    "# TARGET = \"<TU_WPROWADŹ_NAZWĘ_KOLUMNY_CELU>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14f8b9",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Szybki podgląd i czyszczenie\n",
    "Sprawdzimy typy kolumn, braki danych i podstawowe statystyki.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32204d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Typy kolumn:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "print(\"\\nBraki danych w kolumnach:\")\n",
    "display(df.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "display(df.describe(include='all').transpose().head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d3998",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Podział na zbiory i preprocessing\n",
    "- Stratyfikacja względem klasy\n",
    "- Imputacja braków: medianą (num) / najczęstszą (cat)\n",
    "- One-hot encoding dla kategorii (drzewa i RF nie wymagają skalowania, ale pipeline jest ogólny)\n",
    "- **Uwaga na nierównowagę klas** — ustawimy `class_weight='balanced'` w modelach drzewiastych.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05cf6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Oddziel X/y\n",
    "y_raw = df[TARGET]\n",
    "X = df.drop(columns=[TARGET])\n",
    "\n",
    "# Spróbuj konwersji celu na int/kat.\n",
    "if pd.api.types.is_numeric_dtype(y_raw):\n",
    "    # Jeśli wartości nie są 0/1, spróbuj zbinarnizować gdy są tylko 2 unikalne wartości\n",
    "    uniques = np.unique(y_raw.dropna())\n",
    "    if len(uniques) == 2 and set(uniques) != {0,1}:\n",
    "        mapping = {uniques[0]:0, uniques[1]:1}\n",
    "        y = y_raw.map(mapping).astype(int)\n",
    "    else:\n",
    "        y = y_raw.astype(int) if y_raw.dropna().astype(int).equals(y_raw.dropna()) else y_raw.astype(float)\n",
    "else:\n",
    "    y = y_raw.astype('category')\n",
    "\n",
    "# Podział:\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y if len(pd.Series(y).unique())>1 else None, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Rozmiary:\", X_train.shape, X_test.shape)\n",
    "\n",
    "# Wykryj typy kolumn\n",
    "num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # Skalowanie nie jest krytyczne dla drzew, ale zostawiamy dla ogólności (może posłużyć XGB)\n",
    "    ('scaler', StandardScaler(with_mean=False) if len(cat_cols)==0 else StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipe, num_cols),\n",
    "        ('cat', categorical_pipe, cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(f\"Liczba cech numerycznych: {len(num_cols)}, kategorycznych: {len(cat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    proba_available = hasattr(model, \"predict_proba\")\n",
    "    y_proba = model.predict_proba(X_test)[:,1] if proba_available and len(np.unique(y_test))==2 else None\n",
    "\n",
    "    # Dla klasyfikacji wieloklasowej użyj macro-averages\n",
    "    average = 'binary' if len(np.unique(y_test))==2 else 'macro'\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=average, zero_division=0)\n",
    "    roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "\n",
    "    print(f\"\"\"[{name}] \n",
    "Accuracy:  {acc:.4f}\n",
    "Precision: {prec:.4f}\n",
    "Recall:    {rec:.4f}\n",
    "F1-score:  {f1:.4f}\n",
    "ROC-AUC:   {roc:.4f} if roc is not None else '—'\"\"\")\n",
    "\n",
    "    # Macierz pomyłek\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "    plt.title(f\"Confusion Matrix — {name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # ROC & PR (jeśli binarny i mamy proby)\n",
    "    if y_proba is not None:\n",
    "        RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "        plt.title(f\"ROC Curve — {name}\")\n",
    "        plt.show()\n",
    "\n",
    "        PrecisionRecallDisplay.from_predictions(y_test, y_proba)\n",
    "        plt.title(f\"Precision-Recall Curve — {name}\")\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'accuracy': float(acc),\n",
    "        'precision': float(prec),\n",
    "        'recall': float(rec),\n",
    "        'f1': float(f1),\n",
    "        'roc_auc': float(roc) if roc is not None else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe06dec",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Modele\n",
    "### 4.1 Drzewo decyzyjne (baseline)\n",
    "Ustawimy `max_depth=3` (zgodnie z tutorialem) i `class_weight='balanced'` (na wypadek nierównowagi).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ed9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree_pipe = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('clf', DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=RANDOM_STATE))\n",
    "])\n",
    "results = []\n",
    "results.append(evaluate_model(\"Decision Tree (depth=3)\", tree_pipe, X_train, X_test, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ee94c",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Bagging (na bazie drzewa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_tree = DecisionTreeClassifier(class_weight='balanced', random_state=RANDOM_STATE)\n",
    "bag_pipe = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('clf', BaggingClassifier(\n",
    "        estimator=base_tree,\n",
    "        n_estimators=200,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "results.append(evaluate_model(\"Bagging (DecisionTree x200)\", bag_pipe, X_train, X_test, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d468d54",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 Random Forest\n",
    "Również z `class_weight='balanced'`, 200 drzew i maksymalną głębokością 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baaf600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=3, class_weight='balanced',\n",
    "        random_state=RANDOM_STATE, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "results.append(evaluate_model(\"Random Forest (200, depth=3, balanced)\", rf_pipe, X_train, X_test, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3010301",
   "metadata": {},
   "source": [
    "\n",
    "### 4.4 XGBoost (Boosting) — jeśli dostępny\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98633877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if HAS_XGB:\n",
    "    xgb_pipe = Pipeline([\n",
    "        ('prep', preprocess),\n",
    "        ('clf', XGBClassifier(\n",
    "            n_estimators=400, max_depth=3, learning_rate=0.1,\n",
    "            subsample=1.0, colsample_bytree=1.0,\n",
    "            eval_metric='logloss',\n",
    "            scale_pos_weight=None,  # alternatywnie: oblicz wg nierównowagi\n",
    "            random_state=RANDOM_STATE, n_jobs=-1, tree_method='hist'\n",
    "        ))\n",
    "    ])\n",
    "    results.append(evaluate_model(\"XGBoost (400, depth=3)\", xgb_pipe, X_train, X_test, y_train, y_test))\n",
    "else:\n",
    "    print(\"Brak biblioteki xgboost — zainstaluj i uruchom ponownie, by uwzględnić boosting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200f557",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Ważność cech\n",
    "Pokażemy ranking cech dla Random Forest (i XGBoost, jeśli dostępny).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_feature_importances(fitted_pipe, title):\n",
    "    clf = fitted_pipe.named_steps['clf']\n",
    "    prep = fitted_pipe.named_steps['prep']\n",
    "\n",
    "    # Nazwy kolumn po przetwarzaniu\n",
    "    num_cols = prep.transformers_[0][2]\n",
    "    cat_cols = prep.transformers_[1][2] if len(prep.transformers_)>1 else []\n",
    "    ohe = None\n",
    "    try:\n",
    "        ohe = prep.named_transformers_['cat'].named_steps.get('onehot')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if ohe is not None and hasattr(ohe, \"get_feature_names_out\"):\n",
    "        cat_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "    else:\n",
    "        cat_names = list(cat_cols)\n",
    "\n",
    "    feature_names = list(num_cols) + cat_names\n",
    "\n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        importances = clf.feature_importances_\n",
    "        order = np.argsort(importances)[::-1][:30]\n",
    "        plt.figure()\n",
    "        plt.bar(range(len(order)), importances[order])\n",
    "        plt.xticks(range(len(order)), [feature_names[i] if i < len(feature_names) else f\"f{i}\" for i in order], rotation=90)\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Model nie udostępnia feature_importances_.\")\n",
    "\n",
    "# Fit na train, potem wykresy\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "plot_feature_importances(rf_pipe, \"Feature Importance — Random Forest\")\n",
    "\n",
    "if HAS_XGB:\n",
    "    xgb_pipe.fit(X_train, y_train)\n",
    "    plot_feature_importances(xgb_pipe, \"Feature Importance — XGBoost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc50b2",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Porównanie modeli i (opcjonalnie) walidacja krzyżowa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4112f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.sort_values('f1', ascending=False))\n",
    "\n",
    "# (Opcjonalnie) CV dla RF\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "rf_cv = Pipeline([('prep', preprocess), ('clf', RandomForestClassifier(n_estimators=200, max_depth=3, class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1))])\n",
    "cv_scores = cross_val_score(rf_cv, X, y, scoring='f1' if len(np.unique(y))==2 else 'f1_macro', cv=cv, n_jobs=-1)\n",
    "print(\"RF 5-fold CV F1:\", cv_scores.mean().round(4), \"+/-\", cv_scores.std().round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0364aaa",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Wnioski\n",
    "- Drzewo (głębokość 3) to punkt odniesienia — proste i interpretowalne.  \n",
    "- Bagging/Random Forest zwykle poprawiają stabilność i dokładność redukując wariancję.  \n",
    "- Boosting (XGBoost) często osiąga najlepsze wyniki, ale jest wrażliwy na parametry i może przetrenować.  \n",
    "- Sprawdź **feature importance** i porównaj z wiedzą dziedzinową.  \n",
    "- Jeśli klasy są nierówne, patrz także na **F1** i **PR-curve**, nie tylko na accuracy.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
